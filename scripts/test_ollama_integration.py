#!/usr/bin/env python3
"""
Script de prueba para verificar la integraci√≥n con Ollama
"""
import asyncio
import httpx
import json

OLLAMA_URL = "http://localhost:11434"
MODEL = "granite4"

async def test_ollama_connection():
    """Prueba la conexi√≥n con Ollama"""
    print("üîç Verificando conexi√≥n con Ollama...")
    
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{OLLAMA_URL}/api/tags")
            
            if response.status_code == 200:
                models = response.json().get("models", [])
                print(f"‚úÖ Conectado a Ollama")
                print(f"üì¶ Modelos disponibles: {len(models)}")
                for model in models:
                    print(f"   - {model['name']}")
                return True
            else:
                print(f"‚ùå Error al conectar: {response.status_code}")
                return False
                
    except httpx.ConnectError:
        print("‚ùå No se pudo conectar con Ollama")
        print("üí° Aseg√∫rate de que Ollama est√© corriendo: ollama serve")
        return False
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")
        return False

async def test_model_availability():
    """Verifica que el modelo est√© disponible"""
    print(f"\nüîç Verificando modelo {MODEL}...")
    
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{OLLAMA_URL}/api/tags")
            
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m["name"] for m in models]
                
                if MODEL in model_names or f"{MODEL}:latest" in model_names:
                    print(f"‚úÖ Modelo {MODEL} est√° disponible")
                    return True
                else:
                    print(f"‚ùå Modelo {MODEL} no est√° disponible")
                    print(f"üí° Descarga el modelo con: ollama pull {MODEL}")
                    return False
                    
    except Exception as e:
        print(f"‚ùå Error al verificar modelo: {e}")
        return False

async def test_generation():
    """Prueba la generaci√≥n de texto"""
    print(f"\nüîç Probando generaci√≥n de texto...")
    
    prompt = "Resume en 2 oraciones qu√© es la trazabilidad agroalimentaria."
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            print(f"üìù Prompt: {prompt}")
            print("‚è≥ Generando respuesta...")
            
            response = await client.post(
                f"{OLLAMA_URL}/api/generate",
                json={
                    "model": MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": 100
                    }
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                text = result.get("response", "")
                print(f"\n‚úÖ Respuesta generada:")
                print(f"   {text}")
                return True
            else:
                print(f"‚ùå Error en generaci√≥n: {response.status_code}")
                return False
                
    except httpx.TimeoutException:
        print("‚ùå Timeout al generar respuesta")
        print("üí° El modelo puede tardar m√°s en la primera ejecuci√≥n")
        return False
    except Exception as e:
        print(f"‚ùå Error al generar: {e}")
        return False

async def main():
    """Ejecuta todas las pruebas"""
    print("=" * 60)
    print("üß™ Test de Integraci√≥n con Ollama")
    print("=" * 60)
    
    # Test 1: Conexi√≥n
    if not await test_ollama_connection():
        print("\n‚ùå Pruebas fallidas: No se pudo conectar con Ollama")
        return
    
    # Test 2: Modelo disponible
    if not await test_model_availability():
        print("\n‚ùå Pruebas fallidas: Modelo no disponible")
        return
    
    # Test 3: Generaci√≥n
    if not await test_generation():
        print("\n‚ùå Pruebas fallidas: Error en generaci√≥n")
        return
    
    print("\n" + "=" * 60)
    print("‚úÖ Todas las pruebas pasaron exitosamente")
    print("=" * 60)
    print("\nüí° La integraci√≥n con IA est√° lista para usar!")

if __name__ == "__main__":
    asyncio.run(main())
