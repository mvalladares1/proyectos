# Integraci√≥n de IA para Res√∫menes de Trazabilidad

## Descripci√≥n

Este sistema integra Ollama (modelo de IA local) para generar res√∫menes inteligentes de datos de trazabilidad. El usuario puede generar un diagrama de trazabilidad y luego solicitar un resumen contextualizado generado por IA.

## Caracter√≠sticas

- **Res√∫menes contextualizados** seg√∫n el tipo de b√∫squeda:
  - Trazabilidad por proveedor
  - Trazabilidad por rango de fechas
  - Trazabilidad por pallet espec√≠fico
  - Trazabilidad por gu√≠a de despacho
  - Trazabilidad por venta

- **An√°lisis inteligente** que incluye:
  - Flujo completo desde proveedores hasta clientes
  - Vol√∫menes y pesos procesados
  - Procesos de transformaci√≥n
  - Fechas importantes
  - Observaciones relevantes

- **Modelo ligero y r√°pido**: Usa Llama 3.2, optimizado para respuestas r√°pidas

## Instalaci√≥n de Ollama

### Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### macOS

```bash
brew install ollama
```

### Windows

Descarga el instalador desde: https://ollama.com/download/windows

## Configuraci√≥n

### 1. Iniciar el servicio de Ollama

```bash
ollama serve
```

El servicio quedar√° escuchando en `http://localhost:11434`

### 2. Descargar el modelo Llama 3.2

```bash
ollama pull llama3.2
```

Este es un modelo peque√±o (~2GB) optimizado para respuestas r√°pidas.

### Modelos alternativos

Si deseas usar un modelo diferente, puedes modificar el archivo `backend/services/ai_service.py`:

```python
class AIService:
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        self.model = "llama3.2"  # <-- Cambiar aqu√≠
```

Modelos recomendados:
- `llama3.2` - Peque√±o y r√°pido (2GB)
- `llama3.1` - M√°s grande y preciso (4.7GB)
- `mistral` - Alternativa r√°pida (4.1GB)
- `phi3` - Ultra ligero (2.3GB)

## Uso

### 1. En el Dashboard de Trazabilidad

1. Genera un diagrama de trazabilidad usando cualquier modo de b√∫squeda
2. Ver√°s aparecer una nueva secci√≥n **"ü§ñ Resumen Inteligente"**
3. Haz clic en **"‚ú® Generar Resumen"**
4. Espera de 5-10 segundos mientras la IA analiza los datos
5. El resumen aparecer√° en un cuadro informativo

### 2. Tipos de Res√∫menes

#### Por Proveedor
Analiza todas las recepciones, procesos y despachos relacionados con un proveedor en un rango de fechas.

#### Por Rango de Fechas
Proporciona un resumen ejecutivo de toda la actividad en el per√≠odo, incluyendo vol√∫menes, proveedores y clientes.

#### Por Pallet
Traza el historial completo de un pallet espec√≠fico desde su origen hasta su destino final.

#### Por Gu√≠a de Despacho
Resume la composici√≥n y origen de todos los productos en una gu√≠a espec√≠fica.

#### Por Venta
Detalla el origen de los productos vendidos, procesos aplicados y fechas clave.

## Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend   ‚îÇ
‚îÇ  (Streamlit)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ POST /api/v1/containers/traceability/ai-summary
       ‚îÇ { search_context, traceability_data }
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend API ‚îÇ
‚îÇ  (FastAPI)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ generate_traceability_summary()
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Service  ‚îÇ
‚îÇ  (ai_service)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ POST /api/generate
       ‚îÇ { model, prompt, options }
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Ollama     ‚îÇ
‚îÇ  (Local LLM) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Archivos Principales

### Backend

- **`backend/services/ai_service.py`**: Servicio principal de IA
  - Comunicaci√≥n con Ollama
  - Construcci√≥n de prompts contextualizados
  - Extracci√≥n de estad√≠sticas

- **`backend/routers/containers.py`**: Endpoint de API
  - `POST /api/v1/containers/traceability/ai-summary`
  - Validaci√≥n de request
  - Manejo de errores

### Frontend

- **`pages/rendimiento/content.py`**: Integraci√≥n en UI
  - Funci√≥n `render_ai_summary()`
  - Bot√≥n de generaci√≥n
  - Manejo de estado

## Troubleshooting

### Error: "No se pudo conectar con Ollama"

**Soluci√≥n**: Verifica que Ollama est√© corriendo:
```bash
ollama serve
```

### Error: "Modelo no encontrado"

**Soluci√≥n**: Descarga el modelo:
```bash
ollama pull llama3.2
```

### Respuestas muy lentas

**Soluci√≥n**: 
1. Usa un modelo m√°s peque√±o (phi3)
2. Reduce `num_predict` en `ai_service.py`
3. Verifica que tu CPU/GPU sea suficiente

### Respuestas de baja calidad

**Soluci√≥n**:
1. Usa un modelo m√°s grande (llama3.1)
2. Ajusta `temperature` en `ai_service.py`
3. Mejora los prompts en `_build_*_context()`

## Configuraci√≥n Avanzada

### Ajustar par√°metros del modelo

En `backend/services/ai_service.py`:

```python
response = await client.post(
    f"{self.ollama_url}/api/generate",
    json={
        "model": self.model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.3,    # 0.0 = determin√≠stico, 1.0 = creativo
            "top_p": 0.9,          # Muestreo nucleus
            "num_predict": 500,    # M√°ximo de tokens
        }
    }
)
```

### Personalizar prompts

Cada tipo de b√∫squeda tiene su propio m√©todo de construcci√≥n de prompt:

- `_build_sale_context()` - Para ventas
- `_build_date_range_context()` - Para rangos de fechas
- `_build_pallet_context()` - Para pallets
- `_build_guide_context()` - Para gu√≠as
- `_build_generic_context()` - Gen√©rico

## Mejoras Futuras

- [ ] Soporte para streaming de respuestas
- [ ] Cache de res√∫menes previos
- [ ] Exportaci√≥n de res√∫menes a PDF
- [ ] Comparaci√≥n entre per√≠odos
- [ ] Alertas autom√°ticas basadas en IA
- [ ] Integraci√≥n con sistema de notificaciones

## Recursos

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Llama 3.2 Model Card](https://ollama.com/library/llama3.2)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)

